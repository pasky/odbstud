\chapter{The YodaQA System}
\label{ch:work}

My work on Question Answering so far has focused largely on building
a new open source QA platform that is sufficiently generic and modular,
features modern architecture and can serve both as scientific
research testbed and a practical system.
The outcome of this work is the \textbf{YodaQA} system.%
\footnote{Available open source at \url{https://github.com/brmson/yodaqa}
under the Apache Software Licence 2.0.}
\citep{YodaQAPoster2015,YodaQACLEF2015}

\section{YodaQA Pipeline Architecture}
\label{sec:yodaqaarch}

The QA task is implemented in YodaQA as a pipeline that transforms the
question to a set of answers by applying a variety of analysis engines
and annotators.
It is composed from largely independent modules,
allowing easy extension with better algorithms
or novel approaches, while as a fundamental principle
all modules share a common end-to-end pipeline.
The pipeline is implemented in Java

The YodaQA pipeline is implemented mainly in Java,
using the Apache UIMA framework. \citep{UIMA}
YodaQA represents each artifact
as a separate UIMA CAS, allowing easy
parallelization and straightforward
leverage of pre-existing NLP UIMA components;
as a corollary, we compartmentalize
different tasks to interchangeable UIMA annotators.
Extensive support
tooling is included within the package.

The framework is split in several Java packages: \textbf{io} package takes care
of retrieving questions and returning answers, \textbf{pipeline} contains
classes of the general pipeline stages, \textbf{analysis} contains
algorithms for the particular analysis steps, \textbf{provider} has interfaces
to various external resources and \textbf{flow} carries UIMA helper classes
and a web interface dashboard.

\begin{figure}[ht]
\begin{center}
\resizebox{15cm}{!}{\includegraphics{yodaqa-arch.eps}}
%\vspace*{-0.75cm}
\caption{The general architecture of the YodaQA pipeline.  Present but unused final pipeline portions not shown.}
\label{fig:arch}
\end{center}
\end{figure}%\vspace{5mm}

The system maps an input question to ordered list of answer candidates in a pipeline fashion,
with the flow as in Fig.~\ref{fig:arch}, encompassing the following stages:

\begin{itemize}
\item \textbf{Question Analysis} extracts natural language features from
	the input and produces in-system representations of the question.
\item \textbf{Answer Production} generates a set of candidate answers based on the question,
	by performing a \textbf{Primary Search} in the knowledge bases according to the question clues
	and either directly using the results as candidate answers
	or selecting the relevant passages (the \textbf{Passage Extraction})
	and generate candidate answers from these (the \textbf{Passage Analysis}).
\item \textbf{Answer Analysis} generates answer features based on detailed analysis (most importantly, lexical type determination and coercion to question type).
\item \textbf{Answer Merging and Scoring} consolidates the set of answers, removing duplicates and using a machine learned classifier to score answers by their features.
\item \textbf{Successive Refining} (optional) prunes the set of questions in multiple phases, interjecting some extra tasks (evidence diffusion and gathering additional evidence).%
\footnote{We do not include Successive Refining in our evaluation or include further details as it is not beneficial in our current setup.}
\end{itemize}

The basic pipeline flow is much inspired by the Deep\-QA model
of IBM Watson \citep{WatsonPipeline}.  Throughout the flow, answer features
are gradually accumulated and some results of early flow stages (especially
the question analysis) are carried through the rest of the flow.


\section{YodaQA Reference Pipeline}
\label{sec:yodaqarefpip}

The reference pipeline currently considers an English-language task
as outlined in the Introduction --- answering open domain
factoid questions, producing a narrowly phrased answer.
We base the answers on information retrieval from both
unstructured (English Wikipedia --- \textit{enwiki})
and structured (DBpedia \citep{dbpedia}, Freebase \citep{Freebase})
knowledge bases.

In our pipeline, we build on existing third-party NLP analysis tools,
in particular Stanford CoreNLP (Segmenter, POS-Tagger, Parser) \citep{StanfordCoreNLP} \citep{StanfordNNParser},
OpenNLP (Segmenter, NER) \citep{OpenNLP} and LanguageTool (Segmenter, Lemmatizer).%
\footnote{\url{http://www.languagetool.org/}}%
\footnote{Sometimes, different pipeline components default to different
NLP backends to perform the same task, e.g.\ segmentation,
based on empirically determined best fit.}
NLP analysis backends are freely interchangeable thanks
to the DKPro UIMA interface \citep{DKPro}.
For semantic analysis, we also rely heavily on the WordNet lexicon \citep{WordNet}.

Our key design rule is avoidance of hand-crafted rules and heuristics,
instead relying just on fully-learned universal mechanisms;
we use just about 10 hard-coded rules at this point, mostly
in question analysis.

%For a practical illustration of the pipeline processing
%two particular example questions,
%see Appendix~\ref{app:analysis}.


\begin{figure}[hb]
\begin{center}
\resizebox{12cm}{!}{\includegraphics{question-analysis.eps}}
%\vspace*{-0.75cm}
\caption{The general organization of the Question Analysis.}
\label{fig:arch-qa}
\end{center}
\end{figure}%\vspace{5mm}

\subsection{Question Analysis}

The question analysis (Fig.~\ref{fig:arch-qa}) involves
producing a part-of-speech tagging and dependency parse of the question text,
recognizing named entities and
performing entity linking%
\footnote{Right now, entities are linked just by an exact match of the main or alias label.}
to concepts (as represented by \textit{enwiki} articles).
The question representation we produce is similar in spirit
to DeepQA \citep{WatsonQuestion}: a bag-of-features including
a set of clues (keywords, keyphrases and linked concepts),
possible lexical answer types and the selection verb.

\textbf{Clues} represent keywords in the question that determine its content
and are used to query for candidate answers.
Clues based on different question components are assigned different weight
(used in search retrieval and passage extraction, determined empirically) ---
in ascending other, all noun phrases, noun tokens and the selection verb (SV);
the LAT (see below); named entities and matched concepts;
the question sentence subject (determined by dependency parse).

\textbf{Focus} is the center point of the question sentence
indicating the queried object.
Six simple hand-crafted heuristics extract the focus based on the dependency parse.
``name of ---'' constructions are traversed.

\textbf{LAT} (Lexical Answer Type) describes a type of the answer that would fit the question.
This type is not of a pre-defined category but may be an arbitrary English noun,
like in the DeepQA system. \citep{WatsonTyCor}
The LAT is derived from the focus, except question words are mapped to nouns
(``where'' to ``location'', etc.)
and adverbs (like ``hot'') are nominalized (to ``temperature'') using WordNet relations.

\textbf{SV} (Selection Verb) represents the coordinating verb of the
question that selects the answer with regard to other clues (like ``born'',
``received'', etc.)

\begin{figure}[hb]
\begin{center}
\resizebox{15cm}{!}{\includegraphics{answer-production.eps}}
%\vspace*{-0.75cm}
\caption{The general organization of the Answer Producer component --- multiple strategies that generate large amount of answer hypotheses.}
\label{fig:arch-ap}
\end{center}
\end{figure}%\vspace{5mm}

\subsection{Unstructured Knowledge Bases}

The primary source of answers in our QA system is keyword search in free-text knowledge base
(the \textit{enwiki} in our default setting).
While the knowledge base has no formal structure,
we take advantage of the organization of the \textit{enwiki} corpus
where entity descriptions are stored in articles that bear the entity name as title
and the first sentence is typically an informative short description of the entity.
Our search strategies are analogous to basic DeepQA free-text information retrieval methods \citep{WatsonIR}.
We use the Apache Solr\footnote{\url{http://lucene.apache.org/solr/}} search engine (frontend to Apache Lucene).
They are shown as the non-highlighted components in Fig.~\ref{fig:arch-ap}.

\textbf{Title-in-clue search} \citep{WatsonIR} looks for the question clues in the article titles,
essentially aiming to find articles that describe the concepts touched in the question.
The first sentences of the top six articles (which we assume is its summary)
are then used in passage analysis (see below).

\textbf{Full-text search} \citep{WatsonIR} runs a full-text clue search in the article texts and titles,
considering the top six results.
The document texts are split to sentences which are treated as separate passages
and scored based on sum of weights of clues occuring in each passage\footnote{%
The \textit{about-clues} which occur in the document title have their weight divided by four (as determined empirically).}%
\footnote{We also carry an infrastructure for machine learning models scoring candidate passages,
		but they have not been improving performance so far.};
the top three passages from each document are picked for passage analysis.

\textbf{Document search} \citep{WatsonIR} runs a full-text clue search in the article texts;
top 20 article hits are then taken as potential responses,
represented as candidate answers by their titles.

\textbf{Concept search} retrieves articles that have been linked to entities mentioned in the question.
The first sentence as well as passages extracted like in the full-text search are used for passage analysis.

Given a picked passage, the \textbf{passage analysis} process executes an NLP pipeline and generates candidate answers;
currently, the answer extraction strategy entails simply converting all named entities and noun phrases to candidate answers.
Also, object constituents in sentences where subject is the question LAT are converted to candidate answers.

In addition, we implement
an enhanced answer production strategy (inspired by the \textbf{Jacana} QA system)
that approaches the problem of identifying
the answer in a text passage in a way similar to named entity recognition: as
a (token) sequence tagging (by begin-inside-outside labels) that uses the conditional
random field model to predict labels.  \citep{TreeEdit2013Yao}
However, we use a significantly simplified feature set: just part-of-speech tags,
named entity labels and dependency labels as token sequence unigrams, bigrams and trigrams.

\subsection{Structured Knowledge Bases}

Aside of full-text search, we also employ structured knowledge bases organized in RDF triples;
in particular, we query the DBpedia \texttt{ontology} (curated) and \texttt{property} (raw infobox)
namespaces and the Freebase RDF dump.
They are shown as the highlighted components in Fig.~\ref{fig:arch-ap}.

For each concept
linked to an in-question entity, we query for predicates with this concept as a subject%
\footnote{All our knowledge bases are linked to \textit{enwiki}.}
and generate candidate answers for each object in such a triple, with the predicate label seeded as one LAT of the answer.

%For performance reasons, we limit the number of queried Freebase topics to 5 and retrieve only 40 properties per each;
%due to this limitation, we have manually compiled a blacklist of skipped
%``spammy'' properties based on past system behavior analysis
%(e.g.\ location's \textit{people\_born\_here} or music artist's \textit{track}).

Furthermore, we have trained a multi-label classifier (logistic regression)
that predicts \textit{property paths}
likely connecting an identified in-question concept with the answer in the knowledge base
graph based on particular words in question representation. \citep{LeanFreebaseYao}
Howeve, we consider long property paths (similar to the core inferential chain of STAGG \citep{STAGG})
as Freebase is organized such that finding related concepts
often requires traversing intermediate nodes representing relationshibs (e.g.\ siblinghood);
we also do not consider all unigrams and bigrams but just the LATs and SVs.

\subsection{Answer Analysis}

In the answer analysis, the system takes a closer look at the answer snippet
and generates numerous features for each answer.
The dominant task here is type coercion,
i.e.\ checking whether the answer type matches the question LAT.

The answer LAT is produced by multiple strategies:
\begin{itemize}
	\item Answers generated by a named entity recognizer have LAT corresponding to the triggering model;
		we use stock OpenNLP NER models \textit{date}, \textit{location}, \textit{money}, \textit{organization}, \textit{percentage}, \textit{person} and \textit{time}.
	\item Answers containing a number have a generic \textit{quantity} LAT generated.
	\item Answer focuses (the parse tree roots) are looked up in WordNet and \textit{instance-of} pairs are used to generate LATs (e.g.\ \textit{Einstein} is \textit{instance-of} \textit{scientist}).
	\item Answer focuses are looked up in DBpedia and its ontology is used to generate LATs.
	\item Answers originating from a structured knowledge base carry the property name as an LAT.
\end{itemize}

Type coercion between question and answer LATs is performed using the WordNet
hypernymy relation --- i.e.\ \textit{scientist} may be generalized to \textit{person}, or \textit{length} to \textit{quantity}.
We term the type coercion score \textbf{WordNet specificity} and exponentially decrease it
with the number of hypernymy traversals required.
Answer LATs coming from named entity recognizer and quantity are not generalized.
We never generalize further once within the \texttt{noun.Tops} WordNet domain and
based on past behavior analysis, we have manually compiled a further blacklist
of WordNet synsets that are never accepted as coercing generalizations
(e.g. \textit{trait} or \textit{social group}).

The generated features describe the origin of the answer (data source, search result score, clues of which type matched in the passage, distance-based score of adjacent clue occurences, etc.), syntactic overlaps with question clues and type coercion scores (what kind of LATs have been generated, if any type coercion succeeded, what is the WordNet specificity and whether either LAT had to be generalized).

\subsection{Answer Merge-and-Score}

The merging and scoring process also basically follows a simplified DeepQA approach \citep{WatsonScoring}.
Candidate answers of the same text (up to basic normalization, like \textit{the-} removal) are merged;
element-wise maximum is taken as the resulting answer feature vector
(except for the \texttt{\#occurences} feature, where a sum is taken).
To reduce overfitting, too rare features are excluded
(when they occur in less than $1\%$ questions and $0.1\%$ answers).

Supplementary features are produced for each logical feature --- aside of the original value,
a binary feature denoting whether a feature has \textit{not} been generated
and a value normalized over the full set of answers
so that the distribution of the feature values over the answer
has mean 0 and standard deviation 1.
The extended feature vectors are converted to a score $s \in [0,1]$
using a logistic regression classifier.%
\footnote{An alternative gradient-boosted decision forest classifier is also
	available, but it is not beneficial in the default evaluation scenario yet.}
The weight vector is trained on the gold standard of a training dataset,
employing L2 regularization objective.  To strike a good precision-recall
balance, positive answers (which are about $p=0.03$ portion of the total)
are weighed by $0.5/p$.

\subsection{Successive Refining}

The pipeline contains support for additional refining and scoring phases.
By default, after initial answer scoring,
only the top 25 answers are kept with the intent of reducing noise for the next answer scoring classifier.
Answers are compared and those overlapping syntactically (prefix, suffix, or substring aligned with sub-phrase boundaries)
are subject to evidence diffusion where their scores are used as features of the overlapping answers.
Another answer scoring would be then performed, and the answer with the highest score is then finally output by the system.%
\footnote{There is also experimental support for additional evidence gathering phase, where the top 5 answers are looked up using the full-text search together with the question clues, and the number and score of hits are used as additional answer features and final answer rescoring is performed.  Nevertheless, we have not found this approach effective.}

However, while we have found these extra scoring steps beneficial with
weaker pipelines (in particular without the clue overlap features),
in the final pipeline configuration the re-scoring triggers significant
overfitting on the training set and we therefore ignore
the successive refining stage in the benchmarked pipeline.


\section{Results}
\label{sec:results}

As we present evaluation of our system,
we shall first detail our experimental setup;
this also includes discussion of our question dataset.

Then, we proceed with the actual results --- we measure the \textit{AP recall}
of the system (recall of the Answer Production component --- whether a correct answer has been generated and considered,
without regard to its score) and \textit{precision@1} (whether the
correct answer has been returned as the top answer by the system).
We find this preferrable to typical information retrieval measures like MRR or MAP
since in many applications, eventually only the single top answer output by the system
matters; however, we also show the \textit{mean reciprocial rank}
for each configuration and discuss the rank distribution of correct answers.
These measures are further described in Ch.~\ref{ch:survey}.

Aside of the performance of the default configuration, we also discuss
scaling of the system (extending the alotted answer time) and performance
impact of its various components (hold-out testing).

\subsection{Experimental Setup}
\label{sec:expsetup}

Our code is version tracked in a public GitHub repository
\url{https://github.com/brmson/yodaqa}, and the experiments presented
here are based on tag \texttt{v1.2}.
The quality of full-text search is co-determined by Solr version
(we use 4.6.0) and models of the various NLP components which are brought
in by DKPro version 1.7.0.
As for the knowledge bases, we use enwiki-20150112, DBpedia 2014,
Freebase RDF dump from Jan 11 2015, and WordNet 3.1.
Detailed instructions on setting up the same state locally (including
download of the particular dump versions and configuration files) are
distributed along the source code.

An automatic benchmark evaluation system is distributed as part of the
YodaQA software package.  The system evaluates the training and test questions
in parallel and re-trains the machine learning models before scoring the answers.
Therefore, in all the modified system versions considered below, a model trained
specifically for that version is used for scoring answers.

Our benchmark is influenced by two sources of noise.
First, the answer correctness is determined automatically by matching a predefined regex,
but this may yield both false positives and false negatives.%
\footnote{For example numerical quantities with varying formatting and units are notoriously tricky to match by a regular expression.}
Second, during training the models are randomly initialized and therefore their final
performance on a testing set flutters a little.

As a main benchmark of the system performance, we use a dataset of 430 training
and 430 testing open domain factoid questions.
(For system development, exclusively questions from the training set are used.)
This dataset is based on the public question answering benchmark from
the main tasks of the TREC 2001 and 2002 QA tracks
with regular expression answer patterns%
\footnote{\url{http://trec.nist.gov/data/qa/2001_qadata/main_task.html} and 2002 analogically.}
and extended by questions asked
to a YodaQA predecessor by internet users via an IRC interface.
This dataset was further manually reviewed by the author,
ambiguous or outdated questions were removed
and the regex patterns were updated based on current data.
We refer to the resulting 867 question dataset as \texttt{curated} and
randomly split it to the training and testing sets.\footnote{The remaining
7 questions are left unused for now.}

To further facilitate comparison of YodaQA to other systems,
we also benchmark its performance on the (i) original, unrevised and
unabridged TREC datasets
(even though the train/test splits might not be entirely compatible),
and (ii) the WebQuestions dataset \citep{Semantic2013Berant}
(which represents several thousands of open domain factoid questions
tied to Freebase, popular as a semantic parsing benchmark).

\subsection{System Evaluation}

\begin{figure}[ht]
% increase table row spacing, adjust to taste
\renewcommand{\arraystretch}{1.3}
\centering
\begin{tabular}{|c|ccc|}
\hline
Pipeline & AP Recall & Prec@1 & MRR \\ \hline \hline
default & 77.2\% & 31.4\% & 0.409 \\
\hline
full-text scaling ($6\to12$ fetched results) & 80.0\% & 35.3\% & 0.440 \\
enwiki KB hold-out & 42.1\% & 19.8\% & 0.253 \\
structured KB hold-out & 70.7\% & 28.8\% & 0.378 \\
\hline
answer typing hold-out & 77.2\% & 30.7\% & 0.394 \\
concept clues hold-out & 68.1\% & 22.3\% & 0.318 \\
\hline
\end{tabular}
\vspace*{-0.2cm}
\caption{Benchmark results of various pipeline variants on the \textit{curated} test dataset.
MRR is the Mean Reciprocal Rank $|Q|\cdot\sum_{q\in Q}{1/r_q}$.}
\label{fig:holdoutbench}
\end{figure}

\begin{figure}[ht]
\begin{center}
\resizebox{85mm}{!}{\includegraphics{ranks.eps}}
%\vspace*{-0.75cm}
\caption{Number of questions $x$ that output the correct answer ranked at least $y$.}
\label{fig:ranks}
\end{center}
\end{figure}%\vspace{5mm}

Evaluation over various pipeline configurations are laid out in Fig.~\ref{fig:holdoutbench}.
Aside of the general performance of the system,
it is instructive to look at the histogram of answer ranks
for the default pipeline, shown in Fig.~\ref{fig:ranks}.
We can observe that while precision@1 is 31.4\%,
precision@5%
\footnote{Proportion of questions where the correct answer is ranked in top five.}
is already at 50\% of test questions.

The information retrieval parameters of the default pipeline are selected so
that answering a question takes about 10s on average on a single core of
AMD FX-8350 with 24GB RAM and SSD backed databases.%
\footnote{Note that no computation pre-processing was done on the knowledge bases or datasets;
bulk of the time per question is therefore spent querying the search engine and parsing sentences,
making it an accurate representation of time spent on previously unseen questions.}
By raising the limiting
parameters, we can observe further AP recall increase and a solid precision improvement.
Our system could therefore meaningfully make use of further computing resources.

We also benchmarked accuracy with various components of the pipeline
disabled.  We can see that the full-text and structured knowledge bases
are complementary to a degree, but the full-text base is eventually
a much stronger answer source for our system on the open domain factoid
questions of the \texttt{curated} dataset.  The ``answer typing'' hold-out
represents suppression of external resources (Wordnet, DBpedia) when guessing
answer types.  We can see that entity linking in the question
is also an important heuristic for our QA system.

\subsection{Comparison to Other Systems}

\begin{figure}[ht]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
\centering
\begin{tabular}{|c|ccc|}
\hline
System & Precision@1 & F1 & MRR \\ \hline
LLCpass03 \citep{LCC} (hand-crafted system) & 68.5\% & --- & --- \\
AskMSR \citep{AskMSR} (web-search system) & 61.4\% & --- & 0.507 \\ \hline
OpenEphyra \citep{Ephyra2006} (hand-crafted OSS) & ``above 25\%'' & --- & --- \\
JacanaIR \citep{TreeEditIR2013Yao} (modern fully-learned OSS) & --- & 23.1\%$^{*}$ & --- \\
OQA \citep{OQA} (modern fully-learned OSS) & --- & 29\%$^{**}$ & --- \\ \hline
\textbf{YodaQA v1.1} & 26.4\% & 26.4\% & 0.325 \\
\hline
\end{tabular}
\vspace*{-0.2cm}
\caption{Benchmark results of some relevant systems on the unmodified TREC dataset. \\
$^{*}$ MIT99 subset; \quad
$^{**}$ sub-sampled dataset with manual evaluation}
\label{fig:trecbench}
\end{figure}

In Fig.~\ref{fig:trecbench}, we compare various performance measures
with the most relevant previously published systems
on the TREC dataset, as reported in the respective papers,
with ours benchmarked on non-curated version of the TREC 2002, 2003 dataset test split.
This comparison has numerous caveats, though, as explained within the table.
Our system has AP recall 60.9\% on this dataset.

\begin{figure}[ht]
%% increase table row spacing, adjust to taste
%\renewcommand{\arraystretch}{1.3}
\centering
\begin{tabular}{|c|cc|}
\hline
System & F1@1 & F1 (Berant) \\ \hline
Sempre & 35.7\% & 35.7\% \\
JacanaFB & 35.4\% & 33.0\% \\
\textbf{YodaQA v1.1} & 34.3\% & --- \\
\hline
STAGG (state-of-art) & --- & 52.5\% \\
\hline
\end{tabular}
\vspace*{-0.2cm}
\caption{Benchmark results of som relevant systems on the WebQuestions dataset. See also Sec.~\ref{sec:structured}.}
\label{fig:wqbench}
\end{figure}

In Fig.~\ref{fig:wqbench}, we compare ourselves with other published systems
on the WebQuestions dataset (test split), as reported in their respective papers.
We limit our system to use only structured KBs.
Our system has AP recall 78.3\% and MRR 0.431 on this dataset.




\section{Datasets and Other Work}

As part of our work, we published several other outputs besides the YodaQA system itself:

\begin{itemize}
\item The \texttt{curated} TREC-based dataset of factoid questions described above.%
\footnote{\url{https://github.com/brmson/dataset-factoid-curated}}

\item The \texttt{webquestions} dataset that contains WebQuestions dataset in a more convenient format for further processing and automatically annotated by other data such as property paths between questions and answers.%
\footnote{\url{https://github.com/brmson/dataset-factoid-webquestions}}

\item The \texttt{movies} dataset that contains WebQuestions-based dataset for the movie domain, combined with questions based on web user input and feedback on an earlier version of YodaQA.%
\footnote{\url{https://github.com/brmson/dataset-factoid-movies}}
Made available in cooperation with Nguyen Hoang Long.

\item A standalone \texttt{label-lookup} service for fuzzy lookup of labels in corpora.%
\footnote{\url{https://github.com/brmson/label-lookup}}
	Co-authored with Nguyen Hoang Long.
	(A fast Levenshtein distance based lookup library DAWG%
\footnote{\url{https://github.com/brmson/dawg-levenshtein}}
	is a work in progress,
	primarily authored by Tomáš Veselý.)

\item A tool for evaluating the Google QA subsystem.%
\footnote{\url{https://github.com/brmson/google-qa}}
	Primarily authored by Nguyen Hoang Long.
\end{itemize}
