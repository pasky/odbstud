\chapter{Introduction}
\label{ch:intro}

TODO some nice quote

In this report, I would like to propose a doctoral thesis to write
and defend at the Czech Technical University on the topic of question
answering systems.

This thesis proposal is a little unusual since I have radically changed
the topic of my research in the course of my second year of study:
from portfolio-based function optimization to question answering systems.
Therefore, the bulk of my research results published up to now are not
on topic of the proposed thesis.  I shall at least briefly showcase
the main points of my past research in the next section,
after which I exclusively focus at the topic of question answering
and propose a thesis on this topic.

\section{Portfolio-Based Optimization}

The initial focus of my doctoral research
was developing algorithm portfolio strategies
with applications particularly in continuous black-box optimization.
The results of my work have been a software framework for experiments
\cite{COCOpf}
and two results published at top-tier conferences \cite{optpf,ndsqistep}.
This research had been primarily supervised by Dr. Petr Pošík.

Let us consider the problem of finding a minimum value of a continuous
real-parameter function that has inaccessible analytical form.%
\footnote{No analytical form implies that we do not have information
e.g.\ on the derivatives of the function, except approximating
them numerically.}
This is a rich area of research that produced many algorithms over
the last 50 years --- from the venerable Nelder-Mead simplex
algorithm \cite{NM1} to various gradient descent methods to
population-based methods.

\subsection{Online Black-box Algorithm Portfolios}

The key question in the face of such variety of optimization
algorithms is ``which algorithm should I choose?''
Unified comparison benchmarks \cite{COCO1}
can help determining the best option for a particular function class.
However, if a function is truly ``black-box'' and its features
are hard to predict, an automated process with minimum overhead
is certainly desirable.

The problem of algorithm selection is not new \cite{Rice}
and was so far popular mainly when applied to
combinatorial problem solvers \cite{combpfsurvey}.
In my work, I adopted the prism of algorithm portfolios \cite{algportfolios}
with \textit{online} selection.%
\footnote{The concept of offline selection also occurs in the literature,
	when we assume a stream of function instances and apply just a single
	algorithm on each of the instances.}
That is, multiple diverse optimization algorithms are applied
to the given function instance simultaneously, with the best
performers quickly gaining the largest time allocation (that is,
the chance to perform the most optimization steps).

Deciding which algorithm to apply in each step of the portfolio
optimization is essentialy an instance of the well-known Multi-Armed
Bandit Problem, where a policy decides which algorithm to try next
based on their empirically determined expected reward.
I have built a modular Python framework \textbf{COCOpf} \cite{COCOpf}
suitable both for research and application of this problem.

This helped me to identify fine structure of the problem
(particularly, I proposed a classification of functions based on
their in-portfolio behavior).
Further, I have built a reference portfolio of seven well-known
diverse optimization algorithms and based on performance evaluation
using the popular COCO optimization benchmark \cite{COCO1}, I have
identified a policy that significantly improves the baseline and
on well-behaved functions on average overperforms even the overally
best individual algorithm. This result was presented at the
PPSN 2014 conference. \cite{optpf}

\subsection{Minimizing Separable Functions by a Mix of Methods}

Another tier of research on how to best combine different optimization
algorithms concerns speeding up optimization of continuous black-box
\textit{separable} functions in particular.  I have closely cooperated
on this research with my supervisor, Dr. Petr Pošík.

Separable multivariate functions can be decomposed to a sum of univariate
functions, each parametrized solely by a single dimension of the input
vector.
For some very hard separable functions, exploiting separability
is the only way to quickly find the minimum and a natural idea to optimize
such functions is to use univariate optimization algorithms on individual
dimensions.
In \cite{PosikGECCO2009LineSearch}, Brent's method \cite{Brent1973} and the STEP algorithm \cite{STEP} were used to separately optimize the function along each dimension.
Brent's method was shown to be fast in case of unimodal functions, but due to its local nature it fails on multimodal functions.
The global STEP method was able to solve both the uni- and multimodal functions, but needed much larger number of function evaluations.
Moreover, their multidimensional variants were constructed inefficiently: the dimensions were optimized sequentially, one by one.

We have built on the above mentioned methods, and contributed two improvements:

\begin{enumerate}
	\item We combined Brent's method and STEP into a single algorithm which converges faster than STEP (in many cases, it is almost as fast as Brent's method), while it preserves the global search ability of STEP (thus solving a larger proportion of functions than Brent's method, and often doing it faster).

	\item We suggested a better way of making a multidimensional variant of this method. As opposed to solving the 1D problem in all dimensions sequentially, we proposed to interleave the steps in individual dimensions, updating the full coordinates of sampled points based on results obtained in other dimensions so far.
\end{enumerate}

Thus, we have introduced a new hybrid algorithm ``Brent-STEP'' combining
these two methods non-trivially and demonstrated that
on univariate and separable functions the hybrid algorithm
in general outperforms both of them,
in the univariate case often by a wide margin,
and that it is behaving robustly even when one of the constituent methods
is failing to converge.
This result was presented at the GECCO 2015 conference.
\cite{ndsqistep}

\section{Question Answering}

The current focus of my doctoral research
is improving the state-of-art in the field of factoid question answering.
My main results so far have been
building an extensive question answering system \textbf{YodaQA} \cite{YodaQAPoster2015}
and proposing a high-quality dataset \cite{YodaQACLEF2015},
but I have also applied the system to a biological QA domain \cite{YodaQABioASQ2015}
and achieved some new results not published yet (described later in the proposal).
This research is primarily supervised by Dr. Jan Šedivý;
some of the newest results have been achieved while collaborating
with intern students in our group.

\subsection{Factoid Question Answering}

Let us consider the Question Answering problem --- a function of
unstructured user query that returns the information queried for.
This is a harder problem than a linked data graph search (which requires
a precisely structured user query) or a generic search engine (which
returns whole documents or sets of passages instead of the specific
information).

The Question Answering task is however a natural extension of a search
engine, as currently employed e.g.\ in Google Search \cite{googleKG}
or personal assistants like Apple Siri, and with the high
profile IBM Watson Jeopardy! matches \cite{WatsonOverview}
it has became a benchmark of progress in AI research.

My goal is ultimately building a general purpose QA system.
Thus, I consider an ``open domain'' general factoid question answering,
rather than domain-specific applications, though keeping flexibility
in this direction is certainly worthwhile.

TODO some examples of questions and answers

\subsection{Task Outline}

Diverse QA system architectures have been proposed in the last 15 years,
applying different approaches to information retrieval.  A full survey
follows, for now let me outline at least the most basic choices I faced
when designing my system.

First, the restriction to \textbf{factoid} questions means the system
is essentially an extension of a search engine (rather than deducing
facts logically) and should return precisely specified, relatively short
text snippets as short answers.  Answering happens mainly based on fact
lookup.  This is in contrast with different question answering tasks
(like Language Comprehension Entrance Exams) where the system needs to
process a text passage and answer tricky questions about the text meaning
(see Sec.~\ref{sec:nonfactoid}).

Perhaps the most popular approach in factoid QA research has been restricting
the task to querying structured knowledge bases, typically using the
RDF paradigm and accessible via SPARQL\@.  The QA problem can
be then rephrased as learning a function translating free-text user query
to a structured lambda expression or SPARQL query. \cite{Semantic2013Berant, Semantic2014Bordes}
I prefer to focus on unstructured datasets as the coverage of the system
as well as domain versatility increases dramatically; building a combined
portfolio of structured and unstructured knowledge bases
is then again an obvious extension.

When relying on unstructured knowledge bases, a common strategy is to offload
the information retrieval on an external high-quality web search engine
like Google or Bing (see e.g.\ the \textbf{AskMSR} system \cite{AskMSR}
or many others).
I make a point of relying purely on local information sources.%
\footnote{However, as non-benchmarked extension, YodaQA also includes
	Bing search as described in Sec.~\ref{sec:bing}.}
While the task becomes noticeably harder,
I believe the outcome is a more
universal system that could be readily refocused on a specific domain
or proprietary knowledge base, and also a system more appropriate as
a scientific platform as the results are fully reproducible over time.

Finally, a common restriction of the QA problem concerns only selecting
the most relevant answer-bearing passage, given a tuple of input question
and set of pre-selected candidate passages \cite{WangQAGrammar}.
This Answer Sentence Selection task is certainly
worthwhile as a component of a QA system but does not form a full-fledged
system by itself.
It may be argued that returning a whole passage is more useful for the user than a direct narrow answer,
but this precludes any reasoning or other indirect answer synthesis on the part of the system,
while the context and supporting evidence can be still provided by the user interface.
Direct answer output may be also used in a more general AI reasoning engine,
an idea that I keep in sight within my design though it is clearly
out of scope for the thesis I propose.

To summarize, the system I propose should produce short, clear answers
based on information retrieval from both
unstructured (full-text) and structured (database) knowledge bases,
and not rely on any ``omniscient web search'' to make the task easier
specifically in the open domain setting.

\section{This Thesis Proposal}

The rest of this proposal shall focus on building the case for
a thesis on the topic of factoid question answering.
Chapter~\ref{ch:survey}
surveys the field in detail, examining different formulations of the problem
as well as a variety of sub-tasks, reference datasets and approaches,
and recent progress in the field.
Chapter~\ref{ch:work} describes
the work I have done on the thesis so far --- this revolves mainly around
the question answering system YodaQA that I have built, but also progress
on some of the QA sub-tasks.
Chapter~\ref{ch:plan} outlines some of the specific problems of QA
to focus on in the thesis.
I conclude the proposal with a summary in Chapter~\ref{ch:concl}.
