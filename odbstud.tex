% !TEX program = xelatex
\documentclass[11pt,a4paper,notitlepage]{report}
\usepackage{polyglossia}
\usepackage{fontspec}
\usepackage{subfig}
\usepackage{url}
\usepackage{hyperref}
\usepackage[round]{natbib}
\usepackage[nottoc,notlot,notlof]{tocbibind}

% natbib link joining; somewhat breaks \cite, \citet
\makeatletter
\renewcommand\hyper@natlinkbreak[2]{#1}
\makeatother

\usepackage{geometry}
\geometry{%
	includeheadfoot,
	margin=1in
}

\def\TODO{{\bf ??? }}

\title{Systems and Approaches for Question Answering}
\author{Mgr. Petr Baudi≈°}

\begin{document}
\maketitle

\begin{abstract}%
	I define and explore the task of Factoid Question Answering.
	This involves understanding a naturally phrased question
	about some (often open domain) fact, looking this fact up
	in a knowledge base (structured database or unstructured
	text corpus), and scoring the candidate answers.
	I survey the recent scientific work and directions in this
	field, define some interesting sub-tasks and propose a new dataset.
	I then present my own baseline QA pipeline YodaQA that
	combines both structured and unstructured approaches.
	Based on experiences with this baseline and survey of the
	field, I propose some scientifically promising lines of further
	QA research.

	\vspace{3ex}

	\textbf{Errata:} This is a report that was finished in a bit
	of a rush for a deadline, and has numerous imperfections that I dislike
	but couldn't have avoided:

	\begin{itemize}
		\item The survey should be considered preliminary, and
			is further worked on at \url{https://github.com/brmson/qasurvey}.
			Many more unstructured, miscellanous and specialized systems
			should have been described in detail.
			Entity linking should be surveyed.
		\item The description of YodaQA v1.1 contains omissions.
			Gradient-boosted decision forests are currently used
			for answer scoring instead of logistic regression.
			Coarse question classification is used to generate
			extra features for the answer-scoring decision trees.
			Entity linking involves a fuzzy label lookup.
		\item YodaQA v1.1 itself is sub-optimal; the MRR of an improved
			version available at the time of this writing (v1.2)
			is 0.430 on curated, with precision@1 over 35\%.
			The only change between v1.1 and v1.2 was retraining
			of the CRF model for answer production from passages.
		\item YodaQA evaluation on WebQuestions leaves much to be desired.
			While it uses ``only structured sources'', these include
			also DBpedia; restricting this to just Freebase actually
			improves F$_1$@1 and MRR, though it hurts AP recall.
			Also, F$_1$ (Berant) is badly missing,
			but this will require some infrastructure changes in YodaQA
			to handle answer lists in datasets.
	\end{itemize}
\end{abstract}

\tableofcontents

\include{intro}
\include{survey}
\include{work}
\include{plan}
\include{concl}

\appendix
\include{app-opt}
%\include{app-analysis}

\bibliographystyle{plainnat}
\bibliography{qa,opt}

\end{document}
